{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(462, 462)\n",
      "('Accuracy:', 0.625)\n",
      "('Accuracy:', 0.625)\n",
      "('Accuracy:', 0.75)\n",
      "('Accuracy:', 0.75)\n",
      "('Accuracy:', 0.75)\n",
      "('Accuracy:', 0.8125)\n",
      "('Accuracy:', 0.875)\n",
      "('Accuracy:', 0.75)\n",
      "('Accuracy:', 0.875)\n",
      "('Accuracy:', 0.875)\n",
      "('Accuracy:', 1.0)\n",
      "('Accuracy:', 0.8125)\n",
      "('Accuracy:', 0.93333334)\n",
      "('Accuracy:', 0.93333334)\n",
      "('Accuracy:', 1.0)\n",
      "('Accuracy:', 0.86666667)\n",
      "('Accuracy:', 0.86666667)\n",
      "('Accuracy:', 1.0)\n",
      "('Accuracy:', 0.86666667)\n",
      "('Accuracy:', 0.93333334)\n",
      "('Accuracy:', 0.93333334)\n",
      "('Accuracy:', 0.86666667)\n",
      "('Accuracy:', 0.73333335)\n",
      "('Accuracy:', 0.86666667)\n",
      "('Accuracy:', 0.86666667)\n",
      "('Accuracy:', 1.0)\n",
      "('Accuracy:', 0.93333334)\n",
      "('Accuracy:', 0.86666667)\n",
      "('Accuracy:', 0.80000001)\n",
      "('Accuracy:', 0.86666667)\n",
      "0.854444\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "#Step 1 reading data\n",
    "\n",
    "file = r'Heart.csv'\n",
    "data = pd.read_csv('heart.csv')\n",
    "\n",
    "dummies = pd.get_dummies(data['famhist'],prefix='famhist', drop_first=False)\n",
    "data = pd.concat([data,dummies], axis=1)\n",
    "data = data.drop(['famhist'], axis=1)\n",
    "\n",
    "inputs=['sbp','tobacco','ldl','adiposity','typea','obesity','alcohol','age']\n",
    "\n",
    "labels = data['chd']\n",
    "\n",
    "#scaling whole data set\n",
    "\n",
    "for each in inputs:\n",
    "    data[each] = ( data[each] - data[each].min() ) / data[each].max()\n",
    "    \n",
    "features = data.drop(['chd'], axis=1)\n",
    "features.head()\n",
    "\n",
    "features, labels = np.array(features), np.array(labels)\n",
    "print(len(features), len(labels))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "nlabels = np.array(labels)\n",
    "n_epochs=2000\n",
    "n_hidden1 = 200\n",
    "n_hidden2 = 50\n",
    "\n",
    "n_labels= 2\n",
    "n_features = 10\n",
    "\n",
    "def model():\n",
    "    k_fold = KFold(n_splits=30,shuffle=True)\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "\n",
    "    \n",
    "    with tf.name_scope('inputs'):\n",
    "\n",
    "        inputs = tf.placeholder(tf.float32,[None, 10], name ='inputs' )\n",
    "        \n",
    "    with tf.name_scope('target_labels'):\n",
    "        labels = tf.placeholder(tf.int32, [None,], name='output')\n",
    "        labels_one_hot = tf.one_hot(labels, 2)\n",
    "    \n",
    "    with tf.name_scope('weights'):\n",
    "        w_h=init_weights([n_features, n_hidden1])\n",
    "        w_h2=init_weights([n_hidden1, n_hidden2])\n",
    "        w_o =init_weights([n_hidden2, n_labels])\n",
    "        \n",
    "        tf.summary.histogram('hidden_weights1', w_h)\n",
    "        tf.summary.histogram('hidden_weights2', w_h2)\n",
    "        tf.summary.histogram('output_weights', w_o)\n",
    "    \n",
    "    with tf.name_scope('biases'):\n",
    "        b1=tf.Variable(tf.zeros([n_hidden1]))\n",
    "        b2=tf.Variable(tf.zeros([n_hidden2]))\n",
    "        bo=tf.Variable(tf.zeros(n_labels))\n",
    "        \n",
    "        tf.summary.histogram('hidden_biases1', b1)\n",
    "        tf.summary.histogram('hidden_biases2', b2)\n",
    "        tf.summary.histogram('output_biases', bo)\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('hidden_layers'):\n",
    "        inputs=tf.nn.dropout(inputs,0.8)\n",
    "        hidden_layer1 = tf.nn.bias_add(tf.matmul(inputs,w_h), b1)\n",
    "        hidden_layer1 = tf.nn.relu(hidden_layer1)\n",
    "        hidden_layer1=tf.nn.dropout(hidden_layer1,0.5)\n",
    "\n",
    "        hidden_layer2 = tf.nn.bias_add(tf.matmul(hidden_layer1,w_h2), b2)\n",
    "        hidden_layer2 = tf.nn.relu(hidden_layer2)\n",
    "        hidden_layer2=tf.nn.dropout(hidden_layer2,0.5)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    with tf.name_scope('predictions'):\n",
    "\n",
    "        logits = tf.nn.bias_add(tf.matmul(hidden_layer2, w_o), bo, name='logits')\n",
    "        pred = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', pred)\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_one_hot, name='cross_entropy')\n",
    "        cost = tf.reduce_mean(entropy, name='cost')\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cost)\n",
    "        \n",
    "        \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    accuracy_met=[]\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        #tensorboard\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('logs/4', sess.graph)\n",
    "        for train_indices, test_indices in k_fold.split(features):\n",
    "            \n",
    "            train_X = [features[each] for each in train_indices]\n",
    "            train_X=np.array(train_X)\n",
    "            train_Y = [nlabels[each] for each in train_indices]\n",
    "            train_Y=np.array(train_Y)\n",
    "            test_X = [features[each] for each in test_indices]\n",
    "            test_Y = [nlabels[each] for each in test_indices]\n",
    "            test_X=np.array(test_X)\n",
    "            test_Y=np.array(test_Y)\n",
    "            \n",
    "            for i in range(n_epochs):\n",
    "                summary, _, loss = sess.run([merged,optimizer, cost], feed_dict={inputs:train_X, labels:train_Y})\n",
    "                train_writer.add_summary(summary, i+1)\n",
    "                \n",
    "            # Test model\n",
    "            correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(labels_one_hot, 1))\n",
    "            # Calculate accuracy\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            acc=accuracy.eval({inputs: test_X, labels: test_Y})\n",
    "           \n",
    "            print(\"Accuracy:\", acc)\n",
    "            accuracy_met.append(acc)\n",
    "        \n",
    "        \n",
    "        mean=tf.reduce_mean(accuracy_met)\n",
    "        \n",
    "        print mean.eval()\n",
    "        \n",
    "\n",
    "heart_model = model()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
